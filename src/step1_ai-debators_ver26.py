###############################################################################
# step1_ai-debators_ver26.py
#
# Multi-agent courtroom debate runner supporting:
# - Ollama (local) and commercial APIs (OpenAI, Google, xAI) via LiteLLM
# - NLSY97 and COMPAS datasets
#
# This is the ver26 successor to step1_ai-debators-openai_ver25_FREEZE.py.
# The FREEZE file is NOT modified. This file imports from:
#   - llm_client.py  (unified LLM abstraction)
#   - dataset_config.py (dataset configs for NLSY97 and COMPAS)
#   - model_config.py (model list management)
###############################################################################

###############################################################################
# Imports
###############################################################################
import argparse
import asyncio
import enum
import json
import logging
import os
import random
import re
import shutil
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy
import pandas as pd
from pydantic import BaseModel, Field

try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass  # python-dotenv not installed; API keys must be set in environment

from dataset_config import DatasetConfig, get_dataset_config
from llm_client import LLMResponse, create_client
from model_config import ModelSpec, get_model_list

###############################################################################
# Constants (defaults, overridable via CLI)
###############################################################################
FEATURE_ALGO_LS = ["lofo", "mi", "permutation", "shap", "xgboost"]
FEATURE_ALGO_NAME = FEATURE_ALGO_LS[0]
NTOP_SUMMARY_COL = "ntop_text_summary"
ALL_SUMMARY_COL = "all_text_summary"

FLAG_INCL_ID_COL = False

RAND_SELECTION_SEED = 42


###############################################################################
# 1. LogLevel enum for custom logging
###############################################################################
class LogLevel(enum.Enum):
    DEBUG = logging.DEBUG
    INFO = logging.INFO
    WARNING = logging.WARNING
    ERROR = logging.ERROR
    CRITICAL = logging.CRITICAL


###############################################################################
# 2. Global logger
###############################################################################
logger = logging.getLogger(__name__)


###############################################################################
# 3. Data models (copied verbatim from ver25)
###############################################################################
class JudgeOpinion(BaseModel):
    """Track judge's evolving opinion"""
    prediction: str  # YES or NO
    confidence: int = Field(ge=0, le=100)  # 0-100
    timestamp: datetime = Field(default_factory=datetime.now)
    after_speaker: str  # Which speaker this opinion follows
    metadata_api: Optional[Dict[str, Any]] = None  # API metadata (tokens, timing)


class DebateResponse(BaseModel):
    """Structure for debate responses"""
    content: str
    reasoning: Optional[List[str]] = Field(default_factory=list)
    confidence: float = Field(ge=0, le=100)
    critique: Optional[str] = None
    prediction: Optional[str] = "UNKNOWN"

    @staticmethod
    def _strip_markdown_fences(text: str) -> str:
        """Remove markdown code fences (```json ... ```) that some models wrap responses in."""
        stripped = text.strip()
        if stripped.startswith("```"):
            first_nl = stripped.find("\n")
            if first_nl > 0:
                stripped = stripped[first_nl + 1:]
            if stripped.rstrip().endswith("```"):
                stripped = stripped.rstrip()[:-3].rstrip()
        return stripped

    @staticmethod
    def _try_parse_json(text: str) -> Optional[dict]:
        """Try stdlib json.loads, then json_repair for truncated/malformed JSON."""
        json_start = text.find("{")
        json_end = text.rfind("}") + 1

        if json_start < 0:
            return None

        if json_end > json_start:
            try:
                return json.loads(text[json_start:json_end])
            except json.JSONDecodeError:
                pass

        # JSON may be truncated (no closing brace) — try json_repair
        try:
            from json_repair import repair_json
            repaired = repair_json(text[json_start:], return_objects=True)
            if isinstance(repaired, dict):
                return repaired
        except Exception:
            pass

        return None

    @staticmethod
    def _regex_extract_fields(text: str) -> dict:
        """Last-resort regex extraction of individual JSON fields from partial text."""
        import re
        data = {}
        # content — grab the first long string value
        m = re.search(r'"content"\s*:\s*"((?:[^"\\]|\\.){20,})"', text, re.DOTALL)
        if m:
            data["content"] = m.group(1).replace('\\"', '"').replace("\\n", "\n")
        # prediction
        m = re.search(r'"prediction"\s*:\s*"(YES|NO|yes|no|Yes|No)"', text, re.IGNORECASE)
        if m:
            data["prediction"] = m.group(1).upper()
        # confidence
        m = re.search(r'"confidence"\s*:\s*(\d+(?:\.\d+)?)', text)
        if m:
            data["confidence"] = float(m.group(1))
        return data

    @classmethod
    def _build_from_data(cls, data: dict, response_text: str) -> "DebateResponse":
        """Build a DebateResponse from a (possibly partial) data dict."""
        # Normalize reasoning list if present
        if "reasoning" in data and isinstance(data["reasoning"], list):
            data["reasoning"] = [
                str(item) if isinstance(item, str)
                else str(item.get("factor", str(item)))
                for item in data["reasoning"]
            ]

        # Normalize confidence value
        raw_conf = data.get("confidence", 75.0)
        try:
            conf_val = float(raw_conf)
            conf_val = min(max(conf_val, 0.0), 100.0)
        except (ValueError, TypeError):
            conf_val = 75.0

        # Normalize critique (convert list to string if necessary)
        raw_critique = data.get("critique", None)
        if isinstance(raw_critique, list):
            raw_critique = " ".join(raw_critique)

        # Extract prediction
        raw_pred = str(data.get("prediction", "")).upper()
        if raw_pred not in ["YES", "NO"]:
            if "YES" in raw_pred:
                raw_pred = "YES"
            elif "NO" in raw_pred:
                raw_pred = "NO"
            else:
                raw_pred = "UNKNOWN"

        return cls(
            content=data.get("content", response_text),
            reasoning=data.get("reasoning", []),
            confidence=conf_val,
            critique=raw_critique,
            prediction=raw_pred,
        )

    @classmethod
    def parse_response(cls, response_text: str) -> "DebateResponse":
        """Parse raw response text into structured format with multi-layer fallbacks.

        Layers: 1) strip fences + json.loads  2) json_repair  3) regex extraction.
        Always returns a DebateResponse — never fails completely.
        """
        cleaned = cls._strip_markdown_fences(response_text)

        # Layer 1+2: Try JSON parse (stdlib then json_repair)
        data = cls._try_parse_json(cleaned)
        if data:
            return cls._build_from_data(data, response_text)

        # Layer 3: Regex extraction from raw text
        regex_data = cls._regex_extract_fields(response_text)
        if regex_data:
            logger.warning("Used regex fallback to extract fields from malformed response")
            if "content" not in regex_data:
                regex_data["content"] = response_text
            regex_data.setdefault("reasoning", ["Parsed via regex fallback"])
            return cls._build_from_data(regex_data, response_text)

        # Ultimate fallback — use the raw text as content
        return cls(
            content=response_text,
            confidence=75.0,
            reasoning=["Failed to parse structured response"],
        )


class CourtAgent(BaseModel):
    """Enhanced agent with memory and strategy"""
    role: str
    persona: str
    strategy_points: List[str] = Field(default_factory=list)
    memory: List[str] = Field(default_factory=list)

    def update_memory(self, observation: str):
        self.memory.append(observation)

    def add_strategy(self, point: str):
        self.strategy_points.append(point)


###############################################################################
# 4. Utility classes and functions
###############################################################################

class RawResponseLogger:
    """Thread-safe JSONL logger for all raw API responses.

    Saves every API response (successful, malformed, or errored) to a
    datetime-stamped JSONL file for future exploration and recovery.
    One file per run, rotated by datetime in the filename.
    """

    def __init__(self, output_dir: Path):
        self._lock = asyncio.Lock()
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        self._dir = output_dir / "raw_api_responses"
        self._dir.mkdir(parents=True, exist_ok=True)
        self._path = self._dir / f"raw_responses_{ts}.jsonl"
        logger.info(f"Raw API response log: {self._path}")

    async def log(
        self,
        model: str,
        role: str,
        row_identifier: str,
        raw_response: str,
        metadata: Optional[dict] = None,
        parse_status: str = "ok",
        attempt: int = 1,
    ):
        """Append one raw response record to the JSONL file."""
        record = {
            "ts": datetime.now().isoformat(),
            "model": model,
            "role": role,
            "row_id": row_identifier,
            "attempt": attempt,
            "parse_status": parse_status,
            "raw_response": raw_response,
            "metadata": metadata or {},
        }
        line = json.dumps(record, default=str) + "\n"
        async with self._lock:
            with open(self._path, "a", encoding="utf-8") as f:
                f.write(line)


class TranscriptWriter:
    """Handles creation of court transcript"""

    def __init__(self, output_dir: str = "transcripts"):
        self.transcript_entries = []
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True, parents=True)

    def add_entry(self, timestamp: datetime, role: str, content: str,
                  metadata: dict = None, metadata_api: dict = None):
        entry = {
            "timestamp": timestamp.strftime("%Y-%m-%d %H:%M:%S"),
            "role": role,
            "content": content,
            "metadata": metadata or {},
            "metadata_api": metadata_api or {},
        }
        self.transcript_entries.append(entry)

    def write_transcript(self, case_summary: dict = None, row_identifier: str = "",
                         dataset_config: DatasetConfig = None):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"court_transcript_{row_identifier}_{timestamp}.txt".replace(" ", "_")
        filepath = self.output_dir / filename

        with open(filepath, "w", encoding="utf-8") as f:
            f.write(f"COURT TRANSCRIPT\n{'=' * 50}\n\n")

            if case_summary:
                f.write("CASE SUMMARY\n")
                f.write(f"Summary: {case_summary.get('summary', 'N/A')}\n")
                # Use dataset_config labels if available, else fallback
                if dataset_config:
                    for generic_key, label in dataset_config.key_statistics_labels.items():
                        col = dataset_config.key_statistics.get(generic_key, generic_key)
                        f.write(f"{label}: {case_summary.get(col, case_summary.get(generic_key, 'N/A'))}\n")
                else:
                    f.write(f"Age: {case_summary.get('age', 'N/A')}\n")
                    f.write(f"Prior Arrests: {case_summary.get('numberofarrestsby2002', case_summary.get('prior_offenses', 'N/A'))}\n")
                f.write("\n")

            f.write("PROCEEDINGS\n")
            f.write("=" * 50 + "\n\n")

            for entry in self.transcript_entries:
                f.write(f"[{entry['timestamp']}] {entry['role']}\n")
                f.write("-" * 50 + "\n")
                f.write(f"{entry['content']}\n")

                if entry["metadata"]:
                    f.write("\nMetadata:\n")
                    for k, v in entry["metadata"].items():
                        f.write(f"  {k}: {v}\n")
                f.write("\n" + "=" * 50 + "\n\n")

                if entry["metadata_api"]:
                    f.write("\nMetadata API:\n")
                    for k, v in entry["metadata_api"].items():
                        f.write(f"  {k}: {v}\n")
                f.write("\n" + "=" * 50 + "\n\n")

        logger.info(f"Transcript successfully written to {filepath.resolve()}")
        return filepath


def setup_custom_logging(level: LogLevel = LogLevel.INFO, base_dir: str = "logs") -> logging.Logger:
    """Configure custom logging with CLI and file output"""
    global logger
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = Path(base_dir)
    log_dir.mkdir(exist_ok=True)

    logger.handlers.clear()
    logger.setLevel(level.value)

    console_handler = logging.StreamHandler()
    console_handler.setLevel(level.value)
    console_format = logging.Formatter("%(asctime)s | %(levelname)s | %(message)s")
    console_handler.setFormatter(console_format)

    file_handler = logging.FileHandler(
        log_dir / f"log_debate_{timestamp}.txt", encoding="utf-8"
    )
    file_handler.setLevel(logging.DEBUG)
    file_format = logging.Formatter(
        "%(asctime)s | %(levelname)s | %(filename)s:%(lineno)d | %(message)s"
    )
    file_handler.setFormatter(file_format)

    logger.addHandler(console_handler)
    logger.addHandler(file_handler)

    logger.info(f"Logging initialized at {timestamp}")
    logger.info(f"Log file: {log_dir}/log_debate_{timestamp}.txt")
    logger.info(f"Log level: {level.name}")

    return logger


def log_api_interaction(logger: logging.Logger, stage: str, data: dict, meta: dict = None):
    """Helper to log API interactions consistently"""
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    if stage == "REQUEST":
        logger.debug(f"\n{'=' * 50}")
        logger.debug(f"API REQUEST @ {timestamp}")
        logger.debug(f"PROMPT:\n{data.get('prompt', '')}")
    elif stage == "RESPONSE":
        duration = meta.get("duration", 0) if meta else 0
        logger.debug(f"\nAPI RESPONSE @ {timestamp} (took {duration:.2f}s)")
        logger.debug(f"Content:\n{data.get('content', '')}")
        if meta:
            logger.debug("Metadata:")
            for k, v in meta.items():
                logger.debug(f"  {k}: {v}")
        logger.debug(f"{'=' * 50}\n")


def get_features_summary(df: pd.DataFrame, col_new_summary: str,
                         col_features_ls: List[str],
                         feature_descriptions: Dict[str, str]) -> pd.DataFrame:
    """Generate text summary column from feature descriptions."""
    logger.debug(f"Creating summary column: {col_new_summary}")
    logger.debug(f"Using features: {col_features_ls}")

    def generate_summary(row):
        descriptions = []
        for col in col_features_ls:
            description = feature_descriptions.get(col, f"Description not found for {col}")
            value = row.get(col, "N/A")
            descriptions.append(f"{description} is {value}")
        return ", and ".join(descriptions)

    df[col_new_summary] = df.apply(generate_summary, axis=1)
    logger.info(f"new text summary col: {col_new_summary}\n  for col_features_ls: {col_features_ls}")
    return df


def read_all_vignettes(filepath: str, col_features_ls: List[str],
                       ds_config: DatasetConfig) -> pd.DataFrame:
    """Read CSV and generate text summary columns."""
    logger.info(f"Reading all vignettes from: {filepath}")

    df = pd.read_csv(filepath, index_col=None)

    if "Unnamed: 0" in df.columns:
        df.rename(columns={"Unnamed: 0": "id"}, inplace=True)

    if not FLAG_INCL_ID_COL:
        if "id" in df.columns:
            df = df.drop(columns=["id"])
            logger.info("Excluded 'id' column from DataFrame")

    logger.info(f"Loaded {len(df)} total vignettes with columns: {list(df.columns)}")

    # ntop summary
    df = get_features_summary(df, NTOP_SUMMARY_COL, col_features_ls,
                              ds_config.feature_descriptions)
    # all-features summary (exclude summary cols, target, id)
    all_feature_cols = [
        col for col in df.columns
        if col not in [NTOP_SUMMARY_COL, ds_config.target_col, "id"]
    ]
    df = get_features_summary(df, ALL_SUMMARY_COL, all_feature_cols,
                              ds_config.feature_descriptions)

    required_cols = [NTOP_SUMMARY_COL, ALL_SUMMARY_COL, ds_config.target_col]
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"Missing required columns: {missing_cols}")

    return df


def select_vignettes(df_all: pd.DataFrame, strategy: str = "random",
                     row_ct: int = 30, arg_extra: Optional[int] = None) -> pd.DataFrame:
    if strategy == "random":
        seed = arg_extra if arg_extra is not None else RAND_SELECTION_SEED
        logger.info(f"Selecting random {row_ct} rows with seed={seed}")
        df_subset = df_all.sample(n=min(row_ct, len(df_all)), random_state=seed)
    elif strategy == "first-nrows":
        logger.info(f"Selecting first {row_ct} rows.")
        df_subset = df_all.head(row_ct)
    else:
        logger.warning(f"Unknown strategy={strategy}, returning the entire df.")
        df_subset = df_all
    logger.info(f"Selected {len(df_subset)} vignettes (strategy={strategy}).")
    return df_subset


def clean_model_name(model_name: str) -> str:
    cleaned = model_name.strip().lower()
    cleaned = re.sub(r"[:.]+", "_", cleaned)
    cleaned = re.sub(r"[^\w]+", "_", cleaned)
    cleaned = re.sub(r"_+", "_", cleaned)
    cleaned = cleaned.strip("_")
    return cleaned


###############################################################################
# 5. CourtDebateManager
###############################################################################
class CourtDebateManager:
    def __init__(
        self,
        ds_config: DatasetConfig,
        llm_client,
        model_name: str,
        temperature: float = 0.7,
        max_tokens: int = 1024,
        timeout: float = 900.0,
        case_facts_df: Optional[pd.DataFrame] = None,
        row: Optional[pd.Series] = None,
        output_dir: str = "transcripts",
        raw_logger: Optional[RawResponseLogger] = None,
    ):
        self.ds_config = ds_config
        self.llm_client = llm_client
        self.model_name = model_name
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout
        self.raw_logger = raw_logger

        # Build required columns from dataset config
        key_stat_cols = list(ds_config.key_statistics.values())
        required_cols = [NTOP_SUMMARY_COL, ALL_SUMMARY_COL] + key_stat_cols + [ds_config.target_col]

        if row is not None:
            missing_cols = [col for col in required_cols if col not in row.index]
            if missing_cols:
                raise ValueError(f"Row missing required columns: {missing_cols}")

        self.case_facts_df = case_facts_df
        self.debate_history = []
        self.transcript = TranscriptWriter(output_dir=output_dir)
        self.judge_opinions: List[JudgeOpinion] = []

        # Initialize agents
        self.prosecutor = CourtAgent(
            role="Prosecutor",
            persona="""You are an experienced prosecutor focused on public safety. You:
- Emphasize patterns of behavior and risk factors that suggest likely reoffending
- Use statistical evidence and criminology research to support arguments
- Focus on protecting potential future victims
- Maintain professional demeanor while being assertive
- Structure arguments clearly with supporting evidence
- Consider both aggravating circumstances and risk factors""",
        )

        self.defender = CourtAgent(
            role="Defense Attorney",
            persona="""You are a dedicated defense attorney focused on rehabilitation. You:
- Emphasize potential for rehabilitation and positive life changes
- Highlight mitigating circumstances and environmental factors
- Focus on the individual circumstances of your client
- Advocate for alternative interventions to incarceration
- Maintain professional composure while being persuasive
- Structure arguments to address both legal and human aspects""",
        )

        self.judge = CourtAgent(
            role="Judge",
            persona="""You are an experienced judge focused on fair evaluation. You:
- Consider both public safety and rehabilitation potential
- Evaluate arguments based on evidence and legal precedent
- Maintain strict courtroom decorum
- Focus on objective risk assessment
- Consider both statistical and individual factors
- Make decisions based on clear reasoning""",
        )

        # Build selected_case from row using generic keys
        if row is not None:
            self.selected_case = {
                "ntop_text_summary": row[NTOP_SUMMARY_COL],
                "all_text_summary": row[ALL_SUMMARY_COL],
                "target": row[ds_config.target_col],
            }
            # Store key statistics with generic keys
            for generic_key, col_name in ds_config.key_statistics.items():
                self.selected_case[generic_key] = row[col_name]
            # Also store with original column names for backward compat
            for col_name in key_stat_cols:
                if col_name not in self.selected_case:
                    self.selected_case[col_name] = row[col_name]
        else:
            if case_facts_df is not None:
                self.selected_case = self._select_random_case()
            else:
                raise ValueError("Must provide either row or case_facts_df")

    def _select_random_case(self) -> Dict:
        random_row = self.case_facts_df.sample(n=1).iloc[0]
        case = {
            "ntop_text_summary": random_row[NTOP_SUMMARY_COL],
            "all_text_summary": random_row[ALL_SUMMARY_COL],
            "target": random_row[self.ds_config.target_col],
        }
        for generic_key, col_name in self.ds_config.key_statistics.items():
            case[generic_key] = random_row[col_name]
        return case

    def _build_prompt(self, agent: CourtAgent, turn: int) -> str:
        COURTROOM_RULES = """
1. Arguments must be based on presented evidence and facts
2. No personal attacks or inflammatory language
3. Maintain professional courtroom decorum
4. Address specific risk factors and protective factors
5. Consider both statistical and individual factors
6. Focus on future risk prediction, not punishment
7. Respect time limits and turn-taking
8. Present clear, structured arguments
9. Cite relevant research or precedent when available
10. Acknowledge but professionally challenge opposing arguments
"""
        # Build key statistics section dynamically
        key_stats_lines = []
        for generic_key, label in self.ds_config.key_statistics_labels.items():
            value = self.selected_case.get(generic_key, "N/A")
            key_stats_lines.append(f"- {label}: {value}")
        key_stats_section = "\n".join(key_stats_lines)

        task_desc = self.ds_config.task_description

        template = f"""
ROLE: {agent.role}
PERSONA: {agent.persona}

TASK: Evaluate and argue whether the defendant is likely to {task_desc}.

COURTROOM RULES:
{COURTROOM_RULES}

CASE FACTS:
{self.selected_case['ntop_text_summary']}
Detailed Background: {self.selected_case['all_text_summary']}

KEY STATISTICS:
{key_stats_section}

PREVIOUS ARGUMENTS:
{chr(10).join(self.debate_history)}

YOUR STRATEGY POINTS:
{chr(10).join(agent.strategy_points)}

CURRENT TASK:
{"Present opening argument" if turn == 1 else "Respond to previous argument"}
Focus on specific factors that increase or decrease recidivism risk.

Provide your response in JSON format:
{{
    "content": "Your main argument",
    "reasoning": ["<Reason #1>", "<Reason #2>", "<Reason #3>"...],
    "confidence": 0-100,
    "critique": "Self-reflection on argument strength",
    "prediction": "YES or NO"
}}
"""
        return template.strip()

    API_RETRY_MAX = 3  # retries for malformed content (separate from llm_client retries)
    API_RETRY_BACKOFF_BASE = 2.0
    API_RETRY_JITTER_MAX = 1.0

    async def _get_agent_response(
        self,
        agent: CourtAgent,
        prompt: str,
        row_identifier: str = "",
    ) -> Optional[DebateResponse]:
        """Get agent response with retry on malformed content.

        Retries up to API_RETRY_MAX times if the response can't be parsed at all.
        Partial data (e.g. content extracted but no prediction) is accepted and
        returned rather than discarding the whole turn.
        """
        start_time = datetime.now()
        log_api_interaction(logger, "REQUEST", {"prompt": prompt})

        best_response: Optional[DebateResponse] = None
        last_metadata: Optional[dict] = None

        for attempt in range(self.API_RETRY_MAX):
            try:
                llm_response: LLMResponse = await self.llm_client.chat(
                    messages=[
                        {"role": "system", "content": agent.persona},
                        {"role": "user", "content": prompt},
                    ],
                    model=self.model_name,
                )

                duration = llm_response.python_api_duration_sec
                metadata_dict = llm_response.to_metadata_dict()
                last_metadata = metadata_dict
                response_text = llm_response.content

                log_api_interaction(
                    logger, "RESPONSE",
                    {"content": response_text},
                    {"duration": duration, **metadata_dict},
                )

                debate_response = DebateResponse.parse_response(response_text)

                # Retry if prediction is UNKNOWN — even partial extractions
                # (content/confidence found but no YES/NO) become "No Decision"
                # downstream and get filtered out by step3, so it's worth retrying
                is_worth_retrying = debate_response.prediction == "UNKNOWN"

                # Log raw response for future exploration/recovery
                if self.raw_logger:
                    await self.raw_logger.log(
                        model=self.model_name,
                        role=agent.role,
                        row_identifier=row_identifier,
                        raw_response=response_text,
                        metadata=metadata_dict,
                        parse_status="fail" if is_worth_retrying else "ok",
                        attempt=attempt + 1,
                    )

                if is_worth_retrying:
                    # Keep best attempt so far (prefer one with actual content)
                    if best_response is None or len(debate_response.content) > len(best_response.content):
                        best_response = debate_response

                    if attempt < self.API_RETRY_MAX - 1:
                        backoff = self.API_RETRY_BACKOFF_BASE ** (attempt + 1)
                        jitter = random.uniform(0, self.API_RETRY_JITTER_MAX)
                        wait = backoff + jitter
                        logger.warning(
                            f"No YES/NO prediction from {agent.role} (attempt {attempt + 1}/{self.API_RETRY_MAX}), "
                            f"retrying in {wait:.1f}s"
                        )
                        await asyncio.sleep(wait)
                        continue

                    # All retries exhausted — use best partial response
                    logger.warning(
                        f"All {self.API_RETRY_MAX} attempts returned no YES/NO prediction for {agent.role}. "
                        f"Using best partial data."
                    )
                    debate_response = best_response
                    # Fall through to add_entry with partial data

                self.transcript.add_entry(
                    timestamp=datetime.now(),
                    role=agent.role,
                    content=debate_response.content,
                    metadata={
                        "metadata_api": metadata_dict,
                        "prediction": debate_response.prediction,
                        "confidence": debate_response.confidence,
                        "reasoning": debate_response.reasoning,
                        "critique": debate_response.critique,
                    },
                    metadata_api=metadata_dict,
                )

                return debate_response

            except asyncio.TimeoutError:
                logger.warning(f"Timeout for agent {agent.role} (attempt {attempt + 1})")
                if attempt < self.API_RETRY_MAX - 1:
                    await asyncio.sleep(self.API_RETRY_BACKOFF_BASE ** (attempt + 1))
                    continue
                return None
            except Exception as e:
                logger.error(f"Error getting response for {agent.role} (attempt {attempt + 1}): {e}")
                if attempt < self.API_RETRY_MAX - 1:
                    await asyncio.sleep(self.API_RETRY_BACKOFF_BASE ** (attempt + 1))
                    continue
                return None

        return None

    async def _get_silent_judge_opinion(
        self,
        speaker: str,
        latest_argument: str,
    ) -> Optional[JudgeOpinion]:
        """Get judge's current opinion after each speaker.

        Bug fix from ver25: uses self.model_name instead of hardcoded OLLAMA_MODEL_NAME.
        """
        task_desc = self.ds_config.task_description
        prompt = f"""
ROLE: Silent Judge
TASK: Based on the current state of the debate, provide ONLY your current prediction
on whether the defendant will {task_desc}.

CASE SUMMARY:
{self.selected_case[NTOP_SUMMARY_COL]}

LATEST ARGUMENT ({speaker}):
{latest_argument}

PREVIOUS DEBATE HISTORY:
{chr(10).join(self.debate_history)}

Provide your current opinion in JSON format:
{{
    "prediction": "YES or NO",
    "confidence": 0-100
}}
Keep in mind this is just your current opinion, not final ruling.
"""
        import re as _re

        for attempt in range(self.API_RETRY_MAX):
            try:
                llm_response: LLMResponse = await self.llm_client.chat(
                    messages=[
                        {"role": "system", "content": self.judge.persona},
                        {"role": "user", "content": prompt},
                    ],
                    model=self.model_name,
                )

                response_text = llm_response.content
                cleaned = DebateResponse._strip_markdown_fences(response_text)

                # Try structured JSON parse (stdlib + json_repair)
                data = DebateResponse._try_parse_json(cleaned)
                parse_ok = bool(data and "prediction" in data)

                # Log raw response
                if self.raw_logger:
                    await self.raw_logger.log(
                        model=self.model_name,
                        role="SilentJudge",
                        row_identifier=f"after_{speaker}",
                        raw_response=response_text,
                        metadata=llm_response.to_metadata_dict(),
                        parse_status="ok" if parse_ok else "fail",
                        attempt=attempt + 1,
                    )

                api_meta = llm_response.to_metadata_dict()

                if data and "prediction" in data:
                    pred = str(data["prediction"]).upper()
                    if pred not in ("YES", "NO"):
                        pred = "YES" if "YES" in pred else ("NO" if "NO" in pred else None)
                    if pred:  # Only accept if we resolved to YES or NO
                        conf = int(data.get("confidence", 50))
                        conf = min(max(conf, 0), 100)
                        opinion = JudgeOpinion(
                            prediction=pred,
                            confidence=conf,
                            after_speaker=speaker,
                            metadata_api=api_meta,
                        )
                        self.transcript.add_entry(
                            timestamp=datetime.now(),
                            role="SilentJudge",
                            content=f"Opinion after {speaker}: prediction={pred}, confidence={conf}",
                            metadata={
                                "prediction": pred,
                                "confidence": conf,
                                "after_speaker": speaker,
                                "parse_method": "json",
                            },
                            metadata_api=api_meta,
                        )
                        logger.debug(f"Silent judge opinion after {speaker}: {opinion.model_dump_json()}")
                        return opinion
                    # JSON had prediction field but not YES/NO — fall through to regex

                # Regex fallback for prediction/confidence
                pred_match = _re.search(r'"prediction"\s*:\s*"(YES|NO)"', response_text, _re.IGNORECASE)
                conf_match = _re.search(r'"confidence"\s*:\s*(\d+)', response_text)
                if pred_match:
                    pred_val = pred_match.group(1).upper()
                    conf_val = int(conf_match.group(1)) if conf_match else 50
                    opinion = JudgeOpinion(
                        prediction=pred_val,
                        confidence=conf_val,
                        after_speaker=speaker,
                        metadata_api=api_meta,
                    )
                    self.transcript.add_entry(
                        timestamp=datetime.now(),
                        role="SilentJudge",
                        content=f"Opinion after {speaker}: prediction={pred_val}, confidence={conf_val}",
                        metadata={
                            "prediction": pred_val,
                            "confidence": conf_val,
                            "after_speaker": speaker,
                            "parse_method": "regex_fallback",
                        },
                        metadata_api=api_meta,
                    )
                    logger.debug(f"Silent judge opinion (regex) after {speaker}: {opinion.model_dump_json()}")
                    return opinion

                logger.warning(f"No YES/NO judge opinion after {speaker} (attempt {attempt + 1}/{self.API_RETRY_MAX})")
                if attempt < self.API_RETRY_MAX - 1:
                    backoff = self.API_RETRY_BACKOFF_BASE ** (attempt + 1)
                    jitter = random.uniform(0, self.API_RETRY_JITTER_MAX)
                    await asyncio.sleep(backoff + jitter)
                    continue
                return None

            except Exception as e:
                logger.error(f"Error getting judge opinion (attempt {attempt + 1}): {e}")
                if attempt < self.API_RETRY_MAX - 1:
                    await asyncio.sleep(self.API_RETRY_BACKOFF_BASE ** (attempt + 1))
                    continue
                return None

        return None

    async def conduct_debate(self, rounds: int = 3, row_identifier: str = "") -> Dict:
        """Main debate loop."""
        try:
            self.prosecutor.strategy_points = [
                "Focus on number of prior arrests",
                "Identify risk patterns in background",
                "Emphasize public safety concerns",
                "Present statistical recidivism data",
            ]

            self.defender.strategy_points = [
                "Highlight rehabilitation potential",
                "Identify positive life factors",
                "Present alternatives to detention",
                "Address risk factors constructively",
            ]

            for round_num in range(1, rounds + 1):
                logger.info(f"Starting round {round_num}")

                # Prosecution turn
                pros_prompt = self._build_prompt(self.prosecutor, round_num)
                pros_response = await self._get_agent_response(
                    self.prosecutor, pros_prompt, row_identifier=row_identifier,
                )

                if pros_response:
                    self.debate_history.append(f"Prosecutor: {pros_response.content}")
                    self.prosecutor.update_memory(pros_response.content)
                    judge_opinion = await self._get_silent_judge_opinion(
                        "Prosecutor", pros_response.content
                    )
                    if judge_opinion:
                        self.judge_opinions.append(judge_opinion)

                # Defense turn
                def_prompt = self._build_prompt(self.defender, round_num)
                def_response = await self._get_agent_response(
                    self.defender, def_prompt, row_identifier=row_identifier,
                )

                if def_response:
                    self.debate_history.append(f"Defense: {def_response.content}")
                    self.defender.update_memory(def_response.content)
                    judge_opinion = await self._get_silent_judge_opinion(
                        "Defense", def_response.content
                    )
                    if judge_opinion:
                        self.judge_opinions.append(judge_opinion)

            # Judge's final evaluation
            task_desc = self.ds_config.task_description
            judge_prompt = f"""
ROLE: Judge
PERSONA: {self.judge.persona}

CASE SUMMARY:
{self.selected_case[NTOP_SUMMARY_COL]}

DEBATE HISTORY:
{chr(10).join(self.debate_history)}

Based on all arguments presented, provide your final ruling:
1. Will the defendant {task_desc}? (YES/NO)
2. Your confidence level (0-100%)
3. Detailed reasoning for your decision
4. Assessment of key arguments from both sides

Provide your response in JSON format:
{{
    "prediction": "YES or NO",
    "confidence": 0-100,
    "content": "Your detailed explanation and reasoning",
    "reasoning": ["Analysis point 1", "Analysis point 2", ...],
    "critique": "Assessment of arguments from both sides"
}}
"""
            judge_response = await self._get_agent_response(
                self.judge, judge_prompt, row_identifier=row_identifier,
            )

            logger.debug(f"judge_response={judge_response}")

            transcript_path = None
            final_ruling = {
                "prediction": "No decision",
                "confidence": 0,
                "content": "",
                "reasoning": [],
                "critique": None,
            }
            if judge_response:
                self.debate_history.append(
                    f"Judge's Ruling: {judge_response.content}\n{'=' * 30}\n{'=' * 30}"
                )

                final_ruling = {
                    "prediction": judge_response.prediction or "UNKNOWN",
                    "confidence": judge_response.confidence,
                    "content": judge_response.content,
                    "reasoning": judge_response.reasoning,
                    "critique": judge_response.critique,
                }

                # Build case_summary for transcript using key_statistics
                case_summary_for_transcript = {
                    "summary": self.selected_case[NTOP_SUMMARY_COL],
                }
                for generic_key, col_name in self.ds_config.key_statistics.items():
                    case_summary_for_transcript[generic_key] = self.selected_case.get(generic_key, "N/A")
                    case_summary_for_transcript[col_name] = self.selected_case.get(generic_key, "N/A")

                transcript_path = self.transcript.write_transcript(
                    case_summary=case_summary_for_transcript,
                    row_identifier=row_identifier,
                    dataset_config=self.ds_config,
                )
                logger.info(f"Transcript written to: {transcript_path}")

            # Build return dict — backward-compatible for NLSY97
            case_output = {
                "ntop_text_summary": self.selected_case[NTOP_SUMMARY_COL],
                "dataset": self.ds_config.name,
            }

            if self.ds_config.name == "nlsy97":
                # Backward-compatible keys for step2 aggregation
                case_output["age"] = int(self.selected_case.get("age", 0))
                case_output["numberofarrestsby2002"] = int(
                    self.selected_case.get("prior_offenses",
                                           self.selected_case.get("numberofarrestsby2002", 0))
                )
                case_output["y_arrestedafter2002"] = bool(self.selected_case["target"])
            elif self.ds_config.name == "compas":
                case_output["age"] = int(self.selected_case.get("age", 0))
                case_output["priors_count"] = int(
                    self.selected_case.get("prior_offenses",
                                           self.selected_case.get("priors_count", 0))
                )
                case_output["two_year_recid"] = bool(self.selected_case["target"])
            else:
                # Generic fallback
                for generic_key in self.ds_config.key_statistics:
                    case_output[generic_key] = self.selected_case.get(generic_key)
                case_output["target"] = bool(self.selected_case["target"])

            return {
                "case": case_output,
                "debate_history": self.debate_history,
                "judge_opinion_evolution": [
                    {
                        "timestamp": op.timestamp.isoformat(),
                        "after_speaker": op.after_speaker,
                        "prediction": op.prediction,
                        "confidence": op.confidence,
                        "metadata_api": op.metadata_api,
                    }
                    for op in self.judge_opinions
                ],
                "final_ruling": final_ruling,
                "transcript_path": str(transcript_path) if transcript_path else None,
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            logger.error(f"Error in debate: {str(e)}")
            return {
                "error": str(e),
                "final_ruling": {
                    "prediction": "UNKNOWN",
                    "confidence": 0,
                    "content": "",
                    "reasoning": [],
                    "critique": None,
                },
            }


###############################################################################
# 6. CLI argument parsing
###############################################################################
def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="AgenticSimLaw ver26 — Multi-agent courtroom debate runner"
    )
    parser.add_argument(
        "--dataset", choices=["nlsy97", "compas", "compas_nodecile"], default="nlsy97",
        help="Dataset to use (default: nlsy97)",
    )
    parser.add_argument(
        "--ensemble", default="commercial",
        help="Model ensemble name or comma-separated model IDs (default: commercial)",
    )
    parser.add_argument(
        "--cases", type=int, default=30,
        help="Number of cases to process (default: 30)",
    )
    parser.add_argument(
        "--repeats", type=int, default=5,
        help="Repetitions per case/model (default: 5)",
    )
    parser.add_argument(
        "--temperature", type=float, default=0.7,
        help="LLM temperature (default: 0.7)",
    )
    parser.add_argument(
        "--max-tokens", type=int, default=1024,
        help="Max tokens per response (default: 1024)",
    )
    parser.add_argument(
        "--timeout", type=float, default=900.0,
        help="API timeout in seconds (default: 900)",
    )
    parser.add_argument(
        "--dry-run", action="store_true",
        help="Print config + 1 sample prompt, then exit",
    )
    parser.add_argument(
        "--concurrency", type=int, default=1,
        help="Number of concurrent debates per model (default: 1)",
    )
    parser.add_argument(
        "--vignettes", type=str, default=None,
        help="Override path to vignettes CSV (default: from dataset config)",
    )
    parser.add_argument(
        "--output-dir", type=str, default=None,
        help="Resume into existing output directory (skips completed debates)",
    )
    return parser.parse_args()


###############################################################################
# 7. Main entry point
###############################################################################
async def main():
    args = parse_args()

    # Initialize logging
    logger = setup_custom_logging(level=LogLevel.DEBUG)
    logger.info("AgenticSimLaw ver26 — Starting debate runner")
    logger.info(f"Config: dataset={args.dataset}, ensemble={args.ensemble}, "
                f"cases={args.cases}, repeats={args.repeats}, "
                f"temp={args.temperature}, max_tokens={args.max_tokens}")

    # Load dataset config
    ds_config = get_dataset_config(args.dataset)
    logger.info(f"Dataset: {ds_config.display_name} (target={ds_config.target_col})")

    # Load model list
    model_list = get_model_list(args.ensemble)
    logger.info(f"Models ({len(model_list)}): {[m.model_id for m in model_list]}")

    # Get top-n features
    topn = ds_config.default_topn
    feature_importance = ds_config.feature_importance.get(FEATURE_ALGO_NAME, {})
    col_features_ls = [feat[0] for feat in feature_importance.values()][:topn]
    logger.info(f"Top {topn} features ({FEATURE_ALGO_NAME}): {', '.join(col_features_ls)}")

    # Read and select vignettes
    data_dir = Path(__file__).resolve().parent.parent / "data"
    if args.vignettes:
        vignettes_path = Path(args.vignettes)
        if not vignettes_path.is_absolute():
            vignettes_path = Path(__file__).resolve().parent.parent / args.vignettes
        logger.info(f"Using override vignettes: {vignettes_path}")
    else:
        vignettes_path = data_dir / ds_config.vignettes_csv
    df_all = read_all_vignettes(str(vignettes_path), col_features_ls, ds_config)
    df = select_vignettes(df_all, strategy="random", row_ct=args.cases, arg_extra=RAND_SELECTION_SEED)

    # Dry-run mode
    if args.dry_run:
        print("\n=== DRY RUN ===")
        print(f"Dataset: {ds_config.display_name}")
        print(f"Target column: {ds_config.target_col}")
        print(f"Task: {ds_config.task_description}")
        print(f"Features ({topn}): {col_features_ls}")
        print(f"Models: {[m.model_id for m in model_list]}")
        print(f"Cases: {len(df)} | Repeats: {args.repeats}")
        print(f"Temperature: {args.temperature} | Max tokens: {args.max_tokens}")

        # Show sample prompt
        sample_row = df.iloc[0]
        sample_client = create_client(
            model_list[0].model_id, args.temperature, args.max_tokens, args.timeout
        )
        sample_manager = CourtDebateManager(
            ds_config=ds_config,
            llm_client=sample_client,
            model_name=model_list[0].model_id,
            temperature=args.temperature,
            max_tokens=args.max_tokens,
            timeout=args.timeout,
            row=sample_row,
        )
        sample_prompt = sample_manager._build_prompt(sample_manager.prosecutor, 1)
        print(f"\n=== SAMPLE PROSECUTOR PROMPT ===\n{sample_prompt}")
        print("\n=== END DRY RUN ===")
        return

    # Output directory
    if args.output_dir:
        output_base = Path(args.output_dir)
        if not output_base.is_absolute():
            output_base = Path(__file__).resolve().parent.parent / args.output_dir
        logger.info(f"Resuming into existing output directory: {output_base}")
    else:
        datetime_runstart = datetime.now().strftime("%Y%m%d-%H%M%S")
        output_base = Path(__file__).resolve().parent.parent / f"transcripts_ver26_{args.dataset}_{datetime_runstart}"

    # Raw API response logger — datetime-stamped JSONL per run
    raw_logger = RawResponseLogger(output_base)

    concurrency = max(1, args.concurrency)
    semaphore = asyncio.Semaphore(concurrency)
    logger.info(f"Concurrency: {concurrency} debates in parallel across all models")

    # Progress counters (shared across tasks)
    progress = {"done": 0, "skipped": 0, "failed": 0, "total": 0, "start": datetime.now()}

    async def run_single_debate(
        client, original_model_name, safe_model_name, model_output_dir,
        idx, row, attempt_i, ds_config, args,
    ):
        """Run a single debate with semaphore-bounded concurrency."""
        txt_filename = f"transcript_row-{idx}_ver-{attempt_i + 1}.txt"
        json_filename = f"transcript_row-{idx}_ver-{attempt_i + 1}.json"
        txt_path = model_output_dir / txt_filename
        json_path = model_output_dir / json_filename

        # Skip if already exists (restartable)
        if txt_path.exists() or json_path.exists():
            progress["skipped"] += 1
            return

        if NTOP_SUMMARY_COL not in row.index:
            logger.error(f"Missing ntop_text_summary for row={idx}, skipping!")
            progress["failed"] += 1
            return

        async with semaphore:
            try:
                actual_reoffended = row[ds_config.target_col]
                logger.info(f"[concurrent] Starting model={safe_model_name}, row={idx}, ver={attempt_i + 1}")

                # Create manager and run debate
                debate_manager = CourtDebateManager(
                    ds_config=ds_config,
                    llm_client=client,
                    model_name=original_model_name,
                    temperature=args.temperature,
                    max_tokens=args.max_tokens,
                    timeout=args.timeout,
                    row=row,
                    output_dir=str(model_output_dir),
                    raw_logger=raw_logger,
                )

                row_id_str = f"row-{idx}_ver-{attempt_i + 1}"
                result = await debate_manager.conduct_debate(rounds=3, row_identifier=row_id_str)

                # Evaluate correctness
                final_pred_str = result.get("final_ruling", {}).get("prediction", "").strip().upper()
                if "YES" in final_pred_str:
                    predicted_reoffend = True
                elif "NO" in final_pred_str:
                    predicted_reoffend = False
                else:
                    predicted_reoffend = None

                if predicted_reoffend is None:
                    prediction_accurate = None
                else:
                    prediction_accurate = bool(predicted_reoffend) == bool(actual_reoffended)

                result["prediction_accurate"] = prediction_accurate

                # Add correctness entry to transcript
                debate_manager.transcript.add_entry(
                    timestamp=datetime.now(),
                    role="System",
                    content=(
                        f"Judge Prediction: {predicted_reoffend}\n"
                        f"Actual reoffended: {actual_reoffended}\n"
                        f"Prediction accurate: {prediction_accurate}"
                    ),
                    metadata={"correctness": prediction_accurate},
                )

                # Move transcript to final path
                old_transcript_path = result.get("transcript_path", None)
                if old_transcript_path is not None and os.path.exists(old_transcript_path):
                    shutil.move(old_transcript_path, str(txt_path))
                else:
                    # Build case_summary for re-write
                    case_summary_rewrite = {"summary": row[NTOP_SUMMARY_COL]}
                    for gk, col in ds_config.key_statistics.items():
                        case_summary_rewrite[gk] = row[col]
                    re_written_path = debate_manager.transcript.write_transcript(
                        case_summary=case_summary_rewrite,
                        row_identifier=row_id_str,
                        dataset_config=ds_config,
                    )
                    if re_written_path and os.path.exists(re_written_path):
                        shutil.move(str(re_written_path), str(txt_path))

                # Write JSON atomically (temp file + rename) so partial
                # writes on interrupt don't corrupt checkpoint files.
                tmp_json_path = json_path.with_suffix(".json.tmp")
                with open(tmp_json_path, "w", encoding="utf-8") as jf:
                    json.dump(result, jf, indent=2, default=str)
                os.replace(str(tmp_json_path), str(json_path))

                progress["done"] += 1
                elapsed = (datetime.now() - progress["start"]).total_seconds()
                completed = progress["done"]
                remaining = progress["total"] - completed - progress["skipped"]
                if completed > 0 and remaining > 0:
                    eta_sec = (elapsed / completed) * remaining / concurrency
                    eta_min = int(eta_sec // 60)
                    logger.info(
                        f"[progress] {completed + progress['skipped']}/{progress['total']} "
                        f"({completed} new, {progress['skipped']} skipped, {progress['failed']} failed) "
                        f"| ETA: {eta_min}m"
                    )

            except Exception as e:
                progress["failed"] += 1
                logger.error(f"Debate failed model={safe_model_name}, row={idx}, ver={attempt_i + 1}: {e}")

    # Build ALL debate tasks across ALL models in INTERLEAVED order.
    # Interleaving ensures the FIFO semaphore round-robins across providers
    # so we hit 4 independent APIs in parallel, not sequentially.
    model_clients = {}  # model_id -> (client, safe_name, output_dir)
    per_model_tasks = {}  # model_id -> list of (idx, row, attempt_i)

    for model_spec in model_list:
        original_model_name = model_spec.model_id
        safe_model_name = clean_model_name(original_model_name)
        logger.info(f"Preparing model: {safe_model_name} ({model_spec.provider})")

        model_output_dir = output_base / safe_model_name
        model_output_dir.mkdir(parents=True, exist_ok=True)

        client = create_client(
            original_model_name, args.temperature, args.max_tokens, args.timeout
        )
        model_clients[original_model_name] = (client, safe_model_name, model_output_dir)

        tasks_for_model = []
        for idx, row in df.iterrows():
            for attempt_i in range(args.repeats):
                tasks_for_model.append((idx, row, attempt_i))
        per_model_tasks[original_model_name] = tasks_for_model

    # Interleave: take one task from each model in round-robin order
    all_tasks = []
    model_ids = list(per_model_tasks.keys())
    max_tasks_per_model = max(len(v) for v in per_model_tasks.values())
    for i in range(max_tasks_per_model):
        for mid in model_ids:
            if i < len(per_model_tasks[mid]):
                idx, row, attempt_i = per_model_tasks[mid][i]
                client, safe_name, mdir = model_clients[mid]
                all_tasks.append(
                    run_single_debate(
                        client, mid, safe_name, mdir,
                        idx, row, attempt_i, ds_config, args,
                    )
                )

    progress["total"] = len(all_tasks)
    logger.info(
        f"Queued {len(all_tasks)} total debates across {len(model_list)} models "
        f"(interleaved, concurrency={concurrency})"
    )

    # Run ALL debates across ALL models concurrently (bounded by semaphore)
    await asyncio.gather(*all_tasks)

    # Summary per model
    for original_model_name, (_, safe_name, mdir) in model_clients.items():
        done_for_model = len(list(mdir.glob("*.json")))
        logger.info(f"Model {safe_name} complete: {done_for_model} transcripts")

    logger.info("All debates completed successfully.")


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("Program interrupted by user.")
    except Exception as e:
        print(f"Unhandled error: {e}")
    finally:
        logging.shutdown()

# MLOps Guide for Setup and Maintenance

# TODO

- pull all interal globals into centralized config.yaml


# Local Operation:

## NVIDIA:

sudo nvidia-smi -i 1 -pl 300

# Remote Operations

## Runpod.io


# Python Setup

source venv/bin/activate
pip install -r requirements.txt

# Ollama Setup

get latest 
pull models


# Program Execution


## Models

OLLAMA_ENSEMBLE_SIZE_LS = [
x    "llama3.2:1b-instruct-q4_K_M",  # 1400
x    "llama3.2:1b-instruct-fp16",    # 538
x    "llama3.2:3b-instruct-q4_K_M",
x queue/a6000-1   "llama3.2:3b-instruct-fp16",
x w78a6000-1    "llama3.1:8b-instruct-q4_K_M",
x queue/a6000-2   "llama3.1:8b-instruct-fp16",
x rtx    "llama3.1:70b-instruct-q4_K_M",
x a6000   "llama3.3:70b-instruct-q4_K_M",   
x w0a6000/succ    "qwen2.5:0.5b-instruct-q4_K_M",
x rtx3090        "qwen2.5:1.5b-instruct-q4_K_M",
x w0a6000-succ    "qwen2.5:3b-instruct-q4_K_M",
x queue/w0a6000/fail-rtx3090-2    "qwen2.5:7b-instruct-q4_K_M",
x    "qwen2.5:14b-instruct-q4_K_M",
x    "qwen2.5:32b-instruct-q4_K_M", 
O queue/rtx3090-3   "qwen2.5:72b_instruct_q4_K_M",
x    "deepseek-r1_1_5b_q4_K_M",
x w0rtx3090/succ    "deepseek-r1_7b_q4_K_M",
x    "deepseek-r1_8b_q4_K_M",
x w250rtx3090/succ    "deepseek-r1 14b q4"
x a6000    "deepseek-r1:32b",
x -a6000 (Jan22@10:54) queue/a6000-30/300 queue/rtx3090-2    "deepseek-r1:70b",
]

OLLAMA_ENSEMBLE_OSS_LS = [
x    "command-r:35b-08-2024-q4_K_M",
x    "aya-expanse:8b q4",
x    "gemma2:9b-instruct-q4_K_M",
x    "granite3.1-dense:8b-instruct-q4_K_M",
x    "granite3.1-moe:3b-instruct-q4_K_M",
x    "mistral:7b-instruct-q4_K_M",
x    "phi4:14b-q4_K_M",
]
# Add these from previous SIZE ensemble
#     "qwen2.5:7b-instruct-q4_K_M",
#     "llama3.1:8b-instruct-q4_K_M",

OLLAMA_ENSEMBLE_REASONING_LS = [
x    "dolphin3:8b-llama3.1-q4_K_M",
x    "exaone3.5:7.8b-instruct-q4_K_M",
x    "glm4:9b-chat-q4_K_M",
x    "marco-o1:7b-q4_K_M",
x x w250rtx3090/succ    "olmo2:7b-1124-instruct-q4_K_M",
x    "falcon3:7b-instruct-q4_K_M",        # 36
x    "hermes3:8b-llama3.1-q4_K_M",        # 146 (runpod 03:07)
- w250rtx3090/fail    "internlm2:7b-chat-1m-v2.5-q4_K_M",
x    "nemotron-mini:4b-instruct-q4_K_M",  # 0
x    "smallthinker:3b-preview-q4_K_M",    # 104
    "smollm2:1.7b-instruct-q4_K_M",
x    "tulu3:8b-q4_K_M",
    "opencoder:8b-instruct-q4_K_M",      # 8
    "qwen2.5:32b-instruct-q4_K_M",
-    "yi:9b-v1.5-q4_K_M",
]
# Add these from previous SIZE ensemble
#     "qwen2.5:7b-instruct-q4_K_M",
#     "llama3.1:8b-instruct-q4_K_M",
# Add these from previous REASONING ensemble
#     "command-r:35b-08-2024-q4_K_M",
#     "falcon3:7b-instruct-q4_K_M",
#     "gemma2:9b-instruct-q4_K_M",
#     "granite3.1-dense:8b-instruct-q4_K_M",
#     "llama3.1:8b-instruct-q4_K_M",
#     "marco-o1:7b-q4_K_M",
#     "phi4:14b-q4_K_M",
#     "qwen2.5:7b-instruct-q4_K_M",
#     "tulu3:8b-q4_K_M",
# Add the LARGE REASONING models
#     "tulu3:8b-q4_K_M",
#     "qwq:32b-preview-q4_K_M",
#     "qwen2.5:72b-instruct-q4_K_M",
#     "reflection:70b-q4_K_M",
# x    "athene-v2:72b-q4_K_M",



## Inputs


## Outputs

## Files

main.py
decision_ver6.py
models.py
prompt_manager.py
data_manager.py (manager.py?)
metrics.py
performance.py
setup.py
config.py


## Execution

Restartable

## Program Flow

### Config

config.yaml
config_reference.yaml

### STEP 0: Preprocess Data

ibm_nd_eda_Accelerated_Data_Analysis_20250111.ipynb

### STEP 1: Inference

main.py -> 
step1_run_inferences_over_models.py

### STEP 2: Compile Results

evaluation_results_ver3 ->
step2_aggregate_inferences.py

util_filter_csv_data_ver1.py
util_create_text_summary.py
util_aggregate_reports_ver5.py


### STEP 3: Analysis and Visualization

? evaluation_results_ver3

util_visualize_aggregate_reports_ver2.py


### Incomplete:


==========

## Preprocess data

util_insert_link-id_vignettes-ntop.py
to insert required matching 'id' column in both master vignettes_final_clean.cvs and all files in ./top_datasets/ to link then for main.py

python ai-debators-openai_ver16.py



# Cloud Provider

Runpod.io 
H100 PCIe $2.61/hr + 2TB $0.44/hr 
~$0.60/hr idle

# Local Enviornment

- Setup .ssh credentials (runpod.io VM and local ubuntu)
- Setup github local config.email/config.username
- Github pull
- python -m venv venv 
- source venv/bin/activate
- pip install -r requirements.txt

# Setup Remote-SSH VSCode

I'll help you set up VS Code to work with source code on a RunPod VM. Here's how to do it:

First, install the "Remote - SSH" extension in VS Code:

Open VS Code
Go to Extensions (Ctrl+Shift+X)
Search for "Remote - SSH" and install it (by Microsoft)
Get your RunPod instance's SSH connection details:

In the RunPod dashboard, find your pod
Click on "Connect"
Copy the SSH connection string (looks like ssh root@{your-pod-id}.runpod.net -p {port})
Configure VS Code SSH connection:

Press F1 or Ctrl+Shift+P in VS Code
Type "Remote-SSH: Add New SSH Host"
Paste your RunPod SSH connection string
Choose a config file location (usually the default is fine)
Connect to your RunPod instance:

Click the green button in the bottom-left corner of VS Code
Select "Connect to Host..."
Choose your RunPod instance
Enter your password when prompted
Wait for VS Code Server to install on the remote host
Once connected:

Open a folder on the remote machine using File > Open Folder
You can now edit files directly on your RunPod instance
The integrated terminal will run on the remote machine
Extensions you install will be installed on the remote machine
Some tips:

Keep your SSH key in ~/.ssh/config to avoid entering password repeatedly
Install any necessary extensions again in the remote environment
The status bar will show "SSH: your-runpod-instance" when connected
Use the integrated terminal to run commands directly on the RunPod VM
Would you like me to elaborate on any of these steps or explain how to set up specific development tools in the remote environment?

#  Setup Ollama

- Install Ollama.sh
- Pull Models in Ensemble

1/14/2024 Tuesday (EST)
1:55pm - 2:55pm (74 Models)

1/14/2024 Tuesday (EST)
3:04:46pm - 04:10pm  (77 Models)

ollama run CognitiveComputations/dolphin-llama3.1:8b-v2.9.4-Q4_K_M
ollama run CognitiveComputations/dolphin-llama3.1:8b-v2.9.4-Q8_0
ollama run CognitiveComputations/dolphin-mistral-nemo:12b-v2.9.3-Q3_K_M
ollama run CognitiveComputations/dolphin-mistral-nemo:12b-v2.9.3-F16
ollama run CognitiveComputations/dolphin-2.9.2-qwen2-7b:Q4_K_M
ollama run CognitiveComputations/dolphin-2.9.2-qwen2-7b:f16

- Rename dolphin Models

# Copy the model with the new name
ollama cp CognitiveComputations/dolphin-2.9.2-qwen2-7b:f16 dolphin-2.9.2-qwen2-7b:f16
ollama cp CognitiveComputations/dolphin-llama3.1:8b-v2.9.4-Q4_K_M dolphin-llama3.1:8b-v2.9.4-Q4_K_M
ollama cp CognitiveComputations/dolphin-llama3.1:8b-v2.9.4-Q8_0 dolphin-llama3.1:8b-v2.9.4-Q8_0
ollama cp CognitiveComputations/dolphin-mistral-nemo:12b-v2.9.3-Q3_K_M dolphin-mistral-nemo:12b-v2.9.3-Q3_K_M
ollama cp CognitiveComputations/dolphin-mistral-nemo:12b-v2.9.3-F16 dolphin-mistral-nemo:12b-v2.9.3-F16
ollama cp CognitiveComputations/dolphin-2.9.2-qwen2-7b:Q4_K_M dolphin-2.9.2-qwen2-7b:Q4_K_M


ollama cp CognitiveComputations/dolphin-2.9.2-qwen2-7b:f16 dolphin-2.9.2-qwen2-7b:f16
ollama cp CognitiveComputations/dolphin-llama3.1:8b-v2.9.4-Q4_K_M dolphin-llama3.1:8b-v2.9.4-Q4_K_M
ollama cp CognitiveComputations/dolphin-llama3.1:8b-v2.9.4-Q8_0 dolphin-llama3.1:8b-v2.9.4-Q8_0
ollama cp CognitiveComputations/dolphin-mistral-nemo:12b-v2.9.3-Q3_K_M dolphin-mistral-nemo:12b-v2.9.3-Q3_K_M
ollama cp CognitiveComputations/dolphin-mistral-nemo:12b-v2.9.3-F16 dolphin-mistral-nemo:12b-v2.9.3-F16
ollama cp CognitiveComputations/dolphin-2.9.2-qwen2-7b:Q4_K_M dolphin-2.9.2-qwen2-7b:Q4_K_M
ollama list
ollama cp CognitiveComputations/dolphin-mistral-nemo:12b-v2.9.3-Q4_K_M dolphin-mistral-nemo:12b-v2.9.3-Q4_K_M
ollama cp CognitiveComputations/dolphin-mistral-nemo:12b-v2.9.3-Q8_0 dolphin-mistral-nemo:12b-v2.9.3-Q8_0
ollama cp CognitiveComputations/dolphin-2.9.2-qwen2-7b:Q4_K_M dolphin-2.9.2-qwen2-7b:Q4_K_M
ollama cp CognitiveComputations/dolphin-llama3.1:8b-v2.9.4-Q4_K_M dolphin-llama3.1:8b-v2.9.4-Q4_K_M
 

# Remove the old model (optional, only if you want to free up space)
ollama rm CognitiveComputations/dolphin-2.9.2-qwen2-7b:f16
ollama rm CognitiveComputations/dolphin-llama3.1:8b-v2.9.4-Q4_K_M
ollama rm CognitiveComputations/dolphin-llama3.1:8b-v2.9.4-Q8_0
ollama rm CognitiveComputations/dolphin-mistral-nemo:12b-v2.9.3-Q3_K_M
ollama rm CognitiveComputations/dolphin-mistral-nemo:12b-v2.9.3-F16
ollama rm CognitiveComputations/dolphin-2.9.2-qwen2-7b:Q4_K_M


1/14/2024 Tuesday (EST)
1:55pm - 2:55pm

Executing command 52/74: ollama pull phi4:14b-fp16
[GIN] 2025/01/14 - 19:53:04 | 200 |      79.984µs |       127.0.0.1 | HEAD     "/"
time=2025-01-14T19:53:05.175Z level=INFO source=download.go:175 msg="downloading 4ed22bb5bf7f in 30 1 GB part(s)"
time=2025-01-14T19:54:27.435Z level=INFO source=download.go:175 msg="downloading a043cb60f144 in 1 483 B part(s)"
[GIN] 2025/01/14 - 19:54:57 | 200 |         1m53s |       127.0.0.1 | POST     "/api/pull"
Command executed successfully: ollama pull phi4:14b-fp16




(1) Download with leading dir/ then (2) rename without dir/

ollama run CognitiveComputations/dolphin-llama3.1:8b-v2.9.4-Q4_K_M
ollama run CognitiveComputations/dolphin-llama3.1:8b-v2.9.4-Q8_0

ollama run CognitiveComputations/dolphin-mistral-nemo:12b-v2.9.3-Q3_K_M
ollama run CognitiveComputations/dolphin-mistral-nemo:12b-v2.9.3-F16

ollama run CognitiveComputations/dolphin-2.9.2-qwen2-7b:Q4_K_M
ollama run CognitiveComputations/dolphin-2.9.2-qwen2-7b:f16



# Copy the model with the new name
ollama cp CognitiveComputations/dolphin-2.9.2-qwen2-7b:f16 dolphin-2.9.2-qwen2-7b:f16
ollama cp CognitiveComputations/dolphin-llama3.1:8b-v2.9.4-Q4_K_M dolphin-llama3.1:8b-v2.9.4-Q4_K_M
ollama cp CognitiveComputations/dolphin-llama3.1:8b-v2.9.4-Q8_0 dolphin-llama3.1:8b-v2.9.4-Q8_0
ollama cp CognitiveComputations/dolphin-mistral-nemo:12b-v2.9.3-Q3_K_M dolphin-mistral-nemo:12b-v2.9.3-Q3_K_M
ollama cp CognitiveComputations/dolphin-mistral-nemo:12b-v2.9.3-F16 dolphin-mistral-nemo:12b-v2.9.3-F16
ollama cp CognitiveComputations/dolphin-2.9.2-qwen2-7b:Q4_K_M dolphin-2.9.2-qwen2-7b:Q4_K_M

# Remove the old model (optional, only if you want to free up space)
ollama rm CognitiveComputations/dolphin-2.9.2-qwen2-7b:f16
ollama rm CognitiveComputations/dolphin-llama3.1:8b-v2.9.4-Q4_K_M
ollama rm CognitiveComputations/dolphin-llama3.1:8b-v2.9.4-Q8_0
ollama rm CognitiveComputations/dolphin-mistral-nemo:12b-v2.9.3-Q3_K_M
ollama rm CognitiveComputations/dolphin-mistral-nemo:12b-v2.9.3-F16
ollama rm CognitiveComputations/dolphin-2.9.2-qwen2-7b:Q4_K_M


Executing command 6/74: ollama pull dolphin3:8b-llama3.1-fp16
[GIN] 2025/01/14 - 19:21:44 | 200 |      60.617µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/01/14 - 19:21:45 | 200 |  323.934658ms |       127.0.0.1 | POST     "/api/pull"
Command executed successfully: ollama pull dolphin3:8b-llama3.1-fp16


Executing command 7/74: ollama pull dolphin-llama3.1:8b-v2.9.4-Q4_K_M
[GIN] 2025/01/14 - 19:21:45 | 200 |      60.431µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/01/14 - 19:21:45 | 200 |  188.448697ms |       127.0.0.1 | POST     "/api/pull"
Failed to execute command: ollama pull dolphin-llama3.1:8b-v2.9.4-Q4_K_M
pulling manifest
Error: pull model manifest: file does not exist

ollama run CognitiveComputations/dolphin-llama3.1:8b-v2.9.4-Q4_K_M

* ollama run CognitiveComputations/dolphin-llama3.1:8b-v2.9.4-Q8_0

Executing command 8/74: ollama pull dolphin-mistral-nemo:12b-v2.9.3-Q4_K_M
[GIN] 2025/01/14 - 19:21:45 | 200 |      68.112µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/01/14 - 19:21:45 | 200 |    200.5688ms |       127.0.0.1 | POST     "/api/pull"
Failed to execute command: ollama pull dolphin-mistral-nemo:12b-v2.9.3-Q4_K_M
pulling manifest
Error: pull model manifest: file does not exist

ollama run CognitiveComputations/dolphin-mistral-nemo:12b-v2.9.3-Q3_K_M

ollama run CognitiveComputations/dolphin-mistral-nemo:12b-v2.9.3-F16

Executing command 9/74: ollama pull dolphin-2.9.2-qwen2-7b:Q4_K_M
[GIN] 2025/01/14 - 19:21:45 | 200 |      97.039µs |       127.0.0.1 | HEAD     "/"
[GIN] 2025/01/14 - 19:21:46 | 200 |  222.526871ms |       127.0.0.1 | POST     "/api/pull"
Failed to execute command: ollama pull dolphin-2.9.2-qwen2-7b:Q4_K_M
pulling manifest
Error: pull model manifest: file does not exist

ollama run CognitiveComputations/dolphin-2.9.2-qwen2-7b:Q4_K_M

ollama run CognitiveComputations/dolphin-2.9.2-qwen2-7b:f16


GLF:
#1) IDENTIFY LARGE FILES:
(top 10 largest)
git rev-list --objects --all | grep -f <(git verify-pack -v .git/objects/pack/*.idx | sort -k 3 -n | cut -f 1 -d " " | tail -10)

(>100MB)
git rev-list --objects --all | grep -f <(git verify-pack -v .git/objects/pack/*.idx | awk '$3 >= 100*1024*1024 {print $1}') | awk '{print $2 " " int($3/1024/1024) "MB"}'

#2) Remove GLF from git repo:
# Create backup branch just in case
git branch backup-main

# Remove the specific large file from history
git filter-branch --force --index-filter \
  'git rm --cached --ignore-unmatch src/logs/log_debate_20250114_210014.txt' \
  --prune-empty --tag-name-filter cat -- --all

# Clean up the refs and repository
git for-each-ref --format='delete %(refname)' refs/original | git update-ref --stdin
git reflog expire --expire=now --all
git gc --prune=now

#3) ZIP GLF and add to .gitignore

# Zip the logs
cd src/logs
gzip log_debate_20250114_210014.txt
(cd to project root)

# gzip ./data/vignettes_renamed_clean.csv
gzip ./src/logs/log_claude_20250113_170234.txt
gzip ./src/logs/log_claude_20250113_235845.txt
gzip ./src/logs/log_debate_20250114_010643.txt
gzip ./src/logs/log_debate_20250114_013827.txt
gzip ./src/logs/log_debate_20250114_021057.txt
gzip ./src/logs/log_debate_20250114_031228.txt
gzip ./src/logs/log_debate_20250114_083543.txt
gzip ./src/logs/log_debate_20250114_090844.txt
gzip ./src/logs/log_debate_20250114_031550.txt

# Add to .gitignore
echo "src/logs/*.txt" >> .gitignore
echo "src/logs/*.gz" >> .gitignore



Executing command 10/74: ollama pull exaone3.5:32b-instruct-q4_K_M
[GIN] 2025/01/14 - 19:21:46 | 200 |      57.464µs |       127.0.0.1 | HEAD     "/"
time=2025-01-14T19:21:46.359Z level=INFO source=download.go:175 msg="downloading 294fd63925d8 in 1 13 KB part(s)"
time=2025-01-14T19:21:46.588Z level=INFO source=download.go:175 msg="downloading a64d9e642d7b in 1 62 B part(s)"
time=2025-01-14T19:21:47.787Z level=INFO source=download.go:175 msg="downloading 80f503c98d02 in 1 564 B part(s)"
[GIN] 2025/01/14 - 19:21:48 | 200 |  2.745785063s |       127.0.0.1 | POST     "/api/pull"
Command executed successfully: ollama pull exaone3.5:32b-instruct-q4_K_M


/src
==========
(venv) jonc@jonc-MS-7C35:~/code/AI-Debate-Framework/src$ ls -altr
total 1384
-rw-rw-r--  1 jonc jonc  8960 Jan 13 12:28  ai-debators-claude_ver1.py
-rw-rw-r--  1 jonc jonc 10999 Jan 13 12:30  ai-debators-openai_ver1.py
-rw-rw-r--  1 jonc jonc 60171 Jan 13 12:33  debate_log.log
-rw-rw-r--  1 jonc jonc   855 Jan 13 12:34  debate.log
-rw-rw-r--  1 jonc jonc  9805 Jan 13 12:36  ai-debators-gemini_ver1.py
-rw-rw-r--  1 jonc jonc 19601 Jan 13 13:00  courtroom_debate.log
-rw-rw-r--  1 jonc jonc 11679 Jan 13 13:00  ai-debators-claude_ver2.py
-rw-rw-r--  1 jonc jonc 13877 Jan 13 13:09  ai-debators-claude_ver3.py
-rw-rw-r--  1 jonc jonc 19911 Jan 13 13:21  ai-debators-claude_ver4.py
-rw-rw-r--  1 jonc jonc     0 Jan 13 13:24  ai-debators-openai_ver2.py
-rw-rw-r--  1 jonc jonc 23176 Jan 13 13:31  ai-debators-gemini_ver2.py
-rw-rw-r--  1 jonc jonc 20207 Jan 13 13:39  ai-debators-claude_ver5..py
-rw-rw-r--  1 jonc jonc 28707 Jan 13 13:53  ai-debators-claude_ver6.py
-rw-rw-r--  1 jonc jonc 26439 Jan 13 13:59  ai-debators-openai_ver3.py
-rw-rw-r--  1 jonc jonc 28261 Jan 13 15:31  ai-debators-openai_ver4_working.py
-rw-rw-r--  1 jonc jonc 12820 Jan 13 16:17  ai-debators-openai_ver5.py
-rw-rw-r--  1 jonc jonc  1432 Jan 13 16:54  utils-debate.py
-rw-rw-r--  1 jonc jonc 32312 Jan 13 17:02  ai-debators-openai_ver6.py
-rw-rw-r--  1 jonc jonc 32108 Jan 13 17:18  ai-debators-openai_ver7.py
drwxrwxr-x  2 jonc jonc  4096 Jan 13 23:16  debate_results
-rw-rw-r--  1 jonc jonc 33920 Jan 14 02:37  ai-debators-openai_ver9.py
-rw-rw-r--  1 jonc jonc 33420 Jan 14 02:47  ai-debators-openai_ver8.py
-rw-rw-r--  1 jonc jonc 33420 Jan 14 03:15  ai-debators-openai_ver11.py
drwxrwxr-x  4 jonc jonc 12288 Jan 14 09:07  transcripts_old
drwxrwxr-x  2 jonc jonc  4096 Jan 14 09:12  transcripts_openai_ver10-202501140931
-rw-rw-r--  1 jonc jonc 39518 Jan 15 10:35  ai-debators-openai_ver10_working_ref.py
-rw-rw-r--  1 jonc jonc 39651 Jan 15 13:29  ai-debators-openai_ver10_working-orig.py
-rw-rw-r--  1 jonc jonc 42326 Jan 15 13:29  ai-debators-openai_ver10_working_orig.py
-rw-rw-r--  1 jonc jonc 12343 Jan 15 13:29  util-pull-ollama-models_ver5.py
-rw-rw-r--  1 jonc jonc 10526 Jan 15 13:29  util-pull-ollama-models_ver4.py
-rw-rw-r--  1 jonc jonc 10224 Jan 15 13:29  util-pull-ollama-models_ver3.py
-rw-rw-r--  1 jonc jonc  7726 Jan 15 13:29  util-pull-ollama-models_ver2.py
-rw-rw-r--  1 jonc jonc  6905 Jan 15 13:29  util-pull-ollama-models_ver1.py
-rw-rw-r--  1 jonc jonc   154 Jan 15 13:29  util-pull-ollama-models-list.txt
-rw-rw-r--  1 jonc jonc  1993 Jan 15 13:29  util-pull-ollama-models-list_reference.txt
-rw-rw-r--  1 jonc jonc  3101 Jan 15 13:29  util-pull-ollama-models-list_reference_quotes.txt
-rw-rw-r--  1 jonc jonc  6047 Jan 15 13:29  util_ollama-pull-models_ver1.py
-rw-rw-r--  1 jonc jonc 12305 Jan 15 13:29 'util_git _preprocess_lgf_ver2.py'
-rw-rw-r--  1 jonc jonc  9326 Jan 15 13:29 'util_git _preprocess_lgf_ver1.py'
-rw-rw-r--  1 jonc jonc  1135 Jan 15 13:29  ollama_pull_state.json
-rw-rw-r--  1 jonc jonc 17436 Jan 15 13:29  util_pull-ollama-models_ver9.py
-rw-rw-r--  1 jonc jonc 17038 Jan 15 13:29  util_pull-ollama-models_ver8.py
-rw-rw-r--  1 jonc jonc 16983 Jan 15 13:29  util_pull-ollama-models_ver7.py
-rw-rw-r--  1 jonc jonc 15622 Jan 15 13:29  util_pull-ollama-models_ver6.py
-rw-rw-r--  1 jonc jonc 14260 Jan 15 13:29  util_pull-ollama-models_log.txt
-rw-rw-r--  1 jonc jonc 41631 Jan 15 13:39  ai-debators-openai_ver12.py
-rw-rw-r--  1 jonc jonc 39518 Jan 15 13:43  ai-debators-openai_ver10_working_ref2.py
-rw-rw-r--  1 jonc jonc  5527 Jan 15 13:52  util_pull_rename_ollama_models_ver1.py
-rw-rw-r--  1 jonc jonc  6285 Jan 15 14:08  util_pull_rename_ollama_models_ver2.py
-rw-rw-r--  1 jonc jonc   108 Jan 15 14:09  ollama_operations_20250115_140924.log
-rw-rw-r--  1 jonc jonc  1959 Jan 15 14:17  ollama_operations_20250115_141506.log
-rw-rw-r--  1 jonc jonc  8144 Jan 15 14:19  util_pull_rename_ollama_models_ver3.py
-rw-rw-r--  1 jonc jonc  2218 Jan 15 14:19  ollama_operations_20250115_141927.log
-rw-rw-r--  1 jonc jonc  2570 Jan 15 14:22  ollama_operations_20250115_142218.log
-rw-rw-r--  1 jonc jonc  2501 Jan 15 14:22  ollama_operations_20250115_142255.log
-rw-rw-r--  1 jonc jonc  2218 Jan 15 14:30  ollama_operations_20250115_143032.log
-rw-rw-r--  1 jonc jonc  2759 Jan 15 14:31  config_ollama_models.yaml
-rw-rw-r--  1 jonc jonc 43011 Jan 15 14:31  ai-debators-openai_ver10_working.py
-rw-rw-r--  1 jonc jonc 19814 Jan 15 14:37  ollama_operations_20250115_143130.log
-rw-rw-r--  1 jonc jonc 11419 Jan 15 14:39  compute_topn_features_ver1.py
-rw-rw-r--  1 jonc jonc  4743 Jan 15 14:50  compute_topn_features_ver2.py
-rw-rw-r--  1 jonc jonc  2987 Jan 15 15:00  requirements.txt
-rw-rw-r--  1 jonc jonc  5681 Jan 15 15:33  compute_topn_features_ver3.py
-rw-rw-r--  1 jonc jonc 10670 Jan 15 16:26  compute_topn_features_utils_ver1.py
-rw-rw-r--  1 jonc jonc 16085 Jan 15 16:44  compute_topn_features_ver4.py
-rw-rw-r--  1 jonc jonc 16298 Jan 15 16:47  compute_topn_features_utils_ver2.py
-rw-rw-r--  1 jonc jonc 17454 Jan 15 16:51  compute_topn_features_utils.py
drwxrwxr-x  2 jonc jonc  4096 Jan 15 16:51  __pycache__
-rw-rw-r--  1 jonc jonc 16783 Jan 15 16:52  compute_topn_features_ver5.py
-rw-rw-r--  1 jonc jonc 17565 Jan 15 23:37  compute_topn_features_ver6.py
-rw-rw-r--  1 jonc jonc 22600 Jan 16 00:56  compute_topn_features_ver7.py
-rw-rw-r--  1 jonc jonc  5504 Jan 16 01:30  generate_ntop_datasets_ver1.py
-rw-rw-r--  1 jonc jonc   223 Jan 16 01:30  dataset_creation.log
-rw-rw-r--  1 jonc jonc  3567 Jan 16 01:34  dataset_creation_20250116_013438.log
-rw-rw-r--  1 jonc jonc 10336 Jan 16 01:39  generate_ntop_datasets_ver2.py
-rw-rw-r--  1 jonc jonc   344 Jan 16 01:39  dataset_creation_20250116_013942.log
-rw-rw-r--  1 jonc jonc  3407 Jan 16 01:40  dataset_creation_20250116_014027.log
-rw-rw-r--  1 jonc jonc 11051 Jan 16 01:53  dataset_creation_20250116_015330.log
-rw-rw-r--  1 jonc jonc 14629 Jan 16 02:14  dataset_creation_20250116_021443.log
-rw-rw-r--  1 jonc jonc 15056 Jan 16 02:16  dataset_creation_20250116_021652.log
-rw-rw-r--  1 jonc jonc 13262 Jan 16 02:19  generate_ntop_datasets_ver3.py
-rw-rw-r--  1 jonc jonc 14917 Jan 16 02:19  dataset_creation_20250116_021929.log
drwxrwxr-x 17 jonc jonc  4096 Jan 16 02:50  ..
-rw-rw-r--  1 jonc jonc  6851 Jan 16 03:23  ai-debators-openai_ver16.py
-rw-rw-r--  1 jonc jonc     0 Jan 16 03:27  ai-debators-openai_ver15.py
drwxrwxr-x  8 jonc jonc  4096 Jan 16 03:27  .
drwxrwxr-x  2 jonc jonc  4096 Jan 16 03:28  logs
drwxrwxr-x  2 jonc jonc  4096 Jan 16 03:31  transcripts
-rw-rw-r--  1 jonc jonc 44574 Jan 16 13:46  ai-debators-openai_ver14.py